#!/usr/bin/env python3
"""
View samples from Snake HuggingFace datasets generated by generate_dataset_hf.py
"""

import argparse
from collections import Counter
from pathlib import Path

from datasets import load_from_disk
import numpy as np

from render_utils import render_ascii, render_graphical
from pretrain_utils import get_positions_from_state


def parse_indices(indices_str: str, dataset_size: int) -> list:
    """
    Parse index string into list of indices.
    
    Supports:
    - Comma-separated: "0,5,10"
    - Range: "0-10"
    - Single: "5"
    """
    indices = []
    for part in indices_str.split(','):
        part = part.strip()
        if '-' in part:
            start, end = part.split('-')
            indices.extend(range(int(start), int(end) + 1))
        else:
            indices.append(int(part))
    
    # Filter valid indices
    indices = [i for i in indices if 0 <= i < dataset_size]
    return sorted(set(indices))  # Remove duplicates and sort


def show_statistics(dataset, split_name: str):
    """Show dataset statistics."""
    print(f"\n{'='*70}")
    print(f"DATASET STATISTICS - {split_name.upper()}")
    print(f"{'='*70}")
    print(f"Total samples: {len(dataset):,}")
    
    # Action distribution
    actions = dataset['action'][:1000]
    action_counts = Counter(actions)
    action_map = {0: "Up", 1: "Right", 2: "Down", 3: "Left"}
    
    print("\nAction distribution (from first 1000 samples):")
    for action in sorted(action_counts.keys()):
        count = action_counts[action]
        pct = 100 * count / len(actions)
        print(f"  {action} ({action_map[action]}): {count:4d} ({pct:5.1f}%)")

    # Length distribution
    states = dataset['state'][:1000]
    lengths = [len(get_positions_from_state(state)[0]) for state in states]
    print(f"\nSnake length distribution (from first 1000 samples):")
    print(f"  Min: {min(lengths)}, Max: {max(lengths)}, Mean: {np.mean(lengths):.1f}, Median: {np.median(lengths):.1f}")

    # Check for reasoning tokens
    has_reasoning = 'reasoning_tokens' in dataset.features
    print(f"\nReasoning tokens: {'Present' if has_reasoning else 'Not present'}")
    if has_reasoning:
        # Sample a few to get stats
        reasoning_lengths = []
        for i in range(min(100, len(dataset))):
            if 'reasoning_tokens' in dataset[i]:
                tokens = dataset[i]['reasoning_tokens']
                if isinstance(tokens, np.ndarray):
                    reasoning_lengths.append(len(tokens))
                elif isinstance(tokens, list):
                    reasoning_lengths.append(len(tokens))
        
        if reasoning_lengths:
            print(f"  Average length: {np.mean(reasoning_lengths):.1f} tokens")
            print(f"  Min: {min(reasoning_lengths)}, Max: {max(reasoning_lengths)}")
    
    # State shape
    sample = dataset[0]
    state_arr = sample['state']
    print(f"\nState array shape: {state_arr.shape}")
    print(f"State array dtype: {state_arr.dtype}")
    print(f"Action dtype: {type(sample['action']).__name__}")
    
    print(f"{'='*70}\n")


def main():
    parser = argparse.ArgumentParser(
        description="View samples from Snake HuggingFace datasets"
    )
    
    parser.add_argument('--dataset', type=str, required=True, help='Path to dataset directory')
    parser.add_argument('--split', type=str, default='train', choices=['train', 'validation'], help='Which split to view (default: train)')
    parser.add_argument('--indices', type=str, default='0-4', help='Sample indices: comma-separated or range (e.g., "0,5,10" or "0-10", default: "0-4")')
    parser.add_argument('--mode', type=str, default='ascii', choices=['ascii', 'graphical'], help='Display mode (default: ascii)')
    parser.add_argument('--action-names', action='store_true', help='Show action names instead of numbers')
    parser.add_argument('--show-reasoning', action='store_true', help='Display reasoning tokens if present')
    parser.add_argument('--stats', action='store_true', help='Show dataset statistics')
    
    args = parser.parse_args()
    
    # Load dataset
    dataset_path = Path(args.dataset)
    if not dataset_path.exists():
        print(f"Error: Dataset path does not exist: {dataset_path}")
        return
    
    print(f"Loading dataset from {dataset_path}...")
    try:
        dataset_dict = load_from_disk(str(dataset_path))
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return
    
    if args.split not in dataset_dict:
        print(f"Error: Split '{args.split}' not found. Available splits: {list(dataset_dict.keys())}")
        return
    
    dataset = dataset_dict[args.split]
    # Ensure NumPy arrays are returned for tensor-like columns
    try:
        dataset = dataset.with_format('numpy')
    except Exception:
        pass
    
    # Show statistics if requested
    if args.stats:
        show_statistics(dataset, args.split)
    
    # Parse indices
    try:
        indices = parse_indices(args.indices, len(dataset))
    except Exception as e:
        print(f"Error parsing indices: {e}")
        return
    
    if not indices:
        print("No valid indices found")
        return
    
    print(f"\nShowing {len(indices)} sample(s) from {args.split} split:")
    print("="*70)
    
    # Display samples
    for idx in indices:
        if idx >= len(dataset):
            print(f"\nSkipping index {idx} (out of range)")
            continue
        
        sample = dataset[idx]
        state = sample['state']
        action = int(sample['action'])
        
        print(f"\nSample {idx}:")
        print("-"*70)

        sample_reasoning = None
        if 'reasoning' in sample:
            sample_reasoning = sample['reasoning']
        
        # Render based on mode
        if args.mode == 'ascii':
            render_ascii(state, action, args.action_names, sample_reasoning)
        else:
            render_graphical(state, action, args.action_names, sample_reasoning, idx)
        
        print()
    
    print("="*70)


if __name__ == "__main__":
    main()


