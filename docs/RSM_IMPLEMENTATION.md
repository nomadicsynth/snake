# Reasoning Snake Model (RSM) Implementation

## Overview

This implementation adds Chain-of-Thought (CoT) style reasoning to the Snake transformer model, inspired by modern reasoning LLMs like o1. The model now generates explicit reasoning about each possible move before predicting the final action.

## What Changed

### New Files

1. **`reasoning_dsl.py`** - Domain-Specific Language for Snake reasoning
   - Compact text format for describing move outcomes
   - Multi-step lookahead (1-3 steps)
   - Tokenization utilities

### Modified Files

1. **`pretrain_dataset.py`** - Dataset generation with reasoning
   - Added `add_reasoning`, `reasoning_depth`, `reasoning_format` parameters
   - Generates reasoning text for each sample
   - Handles reasoning in failure/epsilon-greedy samples

2. **`pretrain_utils.py`** - Added helper functions
   - `get_positions_from_state()` - Extract snake/food positions from grid

3. **`generate_dataset.py`** - CLI for dataset generation
   - Added `--reasoning`, `--reasoning-depth`, `--reasoning-format` flags
   - Shows RSM mode in output

4. **`snake_jax/network.py`** - Transformer architecture
   - Added `use_reasoning` parameter
   - Reasoning tokens embedded and prepended to grid tokens
   - Backward compatible with non-RSM models

5. **`pretrain_jax.py`** - Training script
   - Auto-detects RSM datasets
   - Handles reasoning tokens in training/validation
   - Saves RSM flag in model config

6. **`play_pretrained.py`** - Inference/visualization
   - Auto-detects RSM models
   - Generates reasoning on-the-fly during play
   - Added `--show-reasoning` flag to print reasoning text

## Reasoning Format

The DSL uses a compact format optimized for transformer learning:

```
THINK: U:wall D:safe(d=5,f=12) L:safe(d=7,f=8) R:death | BEST:D | ACT:1
```

### Components:

- **THINK:** - Reasoning section
  - **U/D/L/R** - Directions (Up/Down/Left/Right)
  - **Outcomes:** `wall`, `death`, `safe`, `apple`
  - **Attributes:**
    - `d=N` - Manhattan distance to food after move
    - `f=N` - Number of free cells (freedom metric)
- **BEST:** - Chosen direction
- **ACT:** - Action index (0-3)

### Multi-step Lookahead:

```
THINK: U:[1:safe->2:safe->3:apple](d=0,f=45) D:[1:death] L:[1:safe->2:wall] R:[1:safe->2:safe->3:safe](d=2,f=50) | BEST:R | ACT:1
```

Shows N steps ahead for each direction, stopping at terminal states.

## Usage

### 1. Generate RSM Dataset

```bash
python generate_dataset.py \
  --num-samples 50000 \
  --reasoning \
  --reasoning-depth 1 \
  --reasoning-format compact \
  --output snake_rsm_dataset.pkl
```

**Parameters:**
- `--reasoning` - Enable RSM mode
- `--reasoning-depth {1,2,3}` - Lookahead steps (default: 1)
- `--reasoning-format {compact,verbose}` - Text format (default: compact)

### 2. Train RSM Model

```bash
python pretrain_jax.py \
  --dataset snake_rsm_dataset.pkl \
  --epochs 20 \
  --batch-size 256 \
  --d-model 128 \
  --num-layers 4
```

The script auto-detects RSM datasets and configures the network accordingly.

### 3. Play with RSM Model

```bash
python play_pretrained.py \
  --model pretrain_models/best_model.pkl \
  --episodes 10 \
  --show-reasoning
```

**Flags:**
- `--show-reasoning` - Print reasoning text during play (RSM models only)

## How It Works: Training vs Inference

### Training (Teacher Forcing)

During training, we use **teacher forcing** with pre-generated expert reasoning:

1. Generate expert reasoning text for each state-action pair
2. Tokenize reasoning to ASCII tokens (0-127)
3. Feed: `[grid_tokens, reasoning_tokens]` → Model → Predict action
4. Loss: Cross-entropy on action prediction only
5. Model learns to map (grid + reasoning) → action

The reasoning is generated by expert heuristics (A* + lookahead), so it's always correct.

### Inference (Autoregressive Generation)

At inference, the model generates its own reasoning autoregressively:

1. Start with grid tokens only
2. Generate reasoning tokens one by one using causal attention
3. Each token attends to: all grid tokens + previous reasoning tokens
4. Stop when model predicts an action token (0-3) instead of reasoning token (4-131)
5. Execute the action

The model learns during training what good reasoning looks like and replicates it at test time.

### Vocabulary Structure

The model's output head predicts over an extended vocabulary:

- **Tokens 0-3**: Action tokens (UP, RIGHT, DOWN, LEFT)
- **Tokens 4-131**: Reasoning tokens (ASCII 0-127, offset by 4)

During generation:

- If model outputs 0-3 → Stop reasoning, execute action
- If model outputs 4-131 → Continue reasoning (subtract 4 for ASCII char)

## Architecture Details

### Token Sequence

For RSM models, the transformer processes:

```
[grid_token_1, ..., grid_token_HxW, reasoning_token_1, ..., reasoning_token_N]
```

- **Grid tokens** (H×W): Flattened spatial grid + 2D positional encoding
- **Reasoning tokens** (up to 128): ASCII-encoded reasoning text, appended after grid

This order matches autoregressive generation: **Grid → CNN features → Reasoning → Action**

### Attention Pattern

**Bidirectional** attention on grid tokens (can see all grid)

**Causal** attention on reasoning tokens (can only see previous reasoning + all grid):

```
Grid tokens:      ████████████  (all-to-all attention)
Reasoning tokens: ████▲▲▲▲▲▲▲▲  (causal: see grid + past reasoning)
```

This is implemented via a custom attention mask in `make_causal_mask()`.

### Embedding

- Grid tokens: `Linear(channels → d_model)` + 2D positional encoding
- Reasoning tokens: `Embedding(vocab_size=128 → d_model)` + learned positional encoding

### Why This Works

1. **Explicit reasoning** helps the model learn strategic thinking
2. **Lookahead** captures multi-step consequences
3. **Structured format** is easier to learn than free-form text
4. **Compact representation** keeps sequence length manageable
5. **Causal order** matches generation: observe grid → generate reasoning → predict action

## Example Reasoning Output

```
State: Snake at (10,10), Food at (8,12), Length=4

THINK: U:safe(d=5,f=142) R:safe(d=1,f=138) D:safe(d=7,f=140) L:death | BEST:R | ACT:1
```

Interpretation:
- **Up**: Safe move, 5 steps to food, 142 free cells
- **Right**: Safe move, 1 step to food (closest!), 138 free cells  
- **Down**: Safe move, 7 steps to food, 140 free cells
- **Left**: Would hit wall/body → death
- **Decision**: Go Right (closest to food while safe)

## Performance Expectations

Based on reasoning model research:

- **Training**: ~20-30% slower due to longer sequences
- **Inference**: Slightly slower (must generate reasoning)
- **Accuracy**: Potentially higher on complex situations requiring lookahead
- **Interpretability**: Much better - can see model's reasoning

## Comparison: Standard vs RSM

| Feature | Standard Model | RSM Model |
|---------|---------------|-----------|
| Input | Grid only | Grid + Reasoning |
| Sequence Length | H×W (400 for 20×20) | 128 + H×W (~528) |
| Interpretable | ❌ | ✅ |
| Training Speed | Fast | Medium |
| Strategic Thinking | Implicit | Explicit |

## Future Improvements

1. **Beam search** during reasoning generation
2. **Learned reasoning** (model generates its own reasoning at inference)
3. **Multi-modal reasoning** (combine symbolic + learned features)
4. **Reasoning verification** (check if reasoning matches action)
5. **Curriculum learning** (start with depth=1, increase to depth=3)

## Notes

- Models are **backward compatible** - standard models work without changes
- RSM flag is auto-detected from dataset and saved in model config
- Reasoning text is regenerated at inference (not stored in state)
- ASCII tokenization (vocab_size=128) is simple but effective

---

**Created:** October 2025  
**Inspired by:** OpenAI o1, Chain-of-Thought reasoning, AlphaGo value networks
