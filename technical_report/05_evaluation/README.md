# 05 - Evaluation

Evaluate trained models through in-game performance testing, next-state prediction accuracy, and behavioral analysis.

## Required Scripts

- `evaluate_baseline.sh` - Test baseline policy in Snake environment
- `evaluate_eqm.sh` - Test EqM models in Snake environment
- `evaluate_all.sh` - Run full evaluation suite on all models
- `compute_next_state_accuracy.py` - Measure world model prediction quality
- `measure_inference_speed.py` - Benchmark sampling speed vs quality
- `analyze_gameplay.py` - Extract game statistics (score, length, death causes)
- `compare_models.py` - Generate model comparison tables
